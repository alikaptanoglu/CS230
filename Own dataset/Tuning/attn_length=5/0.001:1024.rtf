{\rtf1\ansi\ansicpg936\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red25\green25\blue25;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c12941\c12941\c12941;\cssrgb\c100000\c100000\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 ensemble_embedding Parameter sets: [200, 200, 0.001, 0.5, 1, 0.5, 30, 100, 130, 200]\
## Building an RNN model\
Tensor("Unigram/Unigram/embedding_lookup:0", shape=(?, 50, 30), dtype=float32)\
Tensor("Bigram/Bigram/embedding_lookup:0", shape=(?, 50, 100), dtype=float32)\
Tensor("Trigram/Trigram/embedding_lookup:0", shape=(?, 50, 130), dtype=float32)\
Tensor("Fourgram/Fourgram/embedding_lookup:0", shape=(?, 50, 200), dtype=float32)\
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "\
model variables ['Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/biases:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/basic_lstm_cell/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/basic_lstm_cell/biases:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/attn_w:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/attn_v:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/biases:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attn_output_projection/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attn_output_projection/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/basic_lstm_cell/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/basic_lstm_cell/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/attn_w:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/attn_v:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attn_output_projection/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attn_output_projection/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/basic_lstm_cell/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/basic_lstm_cell/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/attn_w:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/attn_v:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attn_output_projection/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attn_output_projection/biases:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/weights:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/biases:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/basic_lstm_cell/weights:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/basic_lstm_cell/biases:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/attn_w:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/attn_v:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/weights:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attention/biases:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attn_output_projection/weights:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/attention_cell_wrapper/attn_output_projection/biases:0', 'Hidden1/Weights:0', 'Hidden1/Biases:0', 'Output/Weights:0', 'Output/Biases:0']\
## Training\
Percent: [####################] 100.00% Finished. tr loss: 17.549, acc1: 0.254, acc5: 0.550\
Training loss: 40.098, acc1: 0.172, acc5: 0.469, ep: 0\
\
Validation loss: 3.037, acc1: 0.420, acc5: 0.746, ep: 0\
Testing loss: 3.143, acc1: 0.400, acc5: 0.735\
\
Model saved ensemble_embedding.model\
Process time per epoch: 1217.716 seconds\
\
Percent: [####################] 100.00% Finished. tr loss: 8.353, acc1: 0.250, acc5: 0.600\
Training loss: 10.648, acc1: 0.256, acc5: 0.602, ep: 1\
\
Validation loss: 2.021, acc1: 0.419, acc5: 0.728, ep: 1\
Testing loss: 2.028, acc1: 0.415, acc5: 0.744\
\
Model saved ensemble_embedding.model\
Process time per epoch: 1199.629 seconds\
\
Percent: [####################] 100.00% Finished. tr loss: 5.357, acc1: 0.312, acc5: 0.625\
Training loss: 6.655, acc1: 0.274, acc5: 0.618, ep: 2\
\
Validation loss: 1.988, acc1: 0.417, acc5: 0.731, ep: 2\
Testing loss: 2.010, acc1: 0.414, acc5: 0.714\
\
Model saved ensemble_embedding.model\
Process time per epoch: 1248.253 seconds\
\
Percent: [###########         ] 53.41%  tr loss: 5.194, acc1: 0.288, acc5: 0.611}