{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN paper test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkLA4xczU6ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the environment 1\n",
        "!pip install gensim "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPGnYedR9ahR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the environment 2\n",
        "!pip install tensorflow==1.0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53LYw_VMrEuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone RNN resources \n",
        "!git clone  https://github.com/weirenorweiren/CS230.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCfdryeoGe6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the working directory to e-t; not to comment directly after '%cd xxx' which could cause error\n",
        "%cd CS230"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY79pRxT_eNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a directory called result in the current working directory\n",
        "!mkdir result "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbSlXfzbUp-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Check the GPU\n",
        "# !nvidia-smi "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5gcX3bUv3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Check the version of tensorflow\n",
        "# !pip freeze | grep -i tensorflow "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ph26Z6Rr-OI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Print (absolute) working directory\n",
        "# !pwd "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTq2F5pDlSX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # List the documents\n",
        "# !ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnEffvB_q31r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Two reasons why we cannot directly run their fxxk model: \n",
        "# 1. The necessary directory 'result' is not automatically created in Colab\n",
        "# 2. Colab can just recognize the absolute path rather than relative path. For example, when passing the flags/parameters, 'result/pred.txt' cannot be recognized so we have to use '/content/ethnicity-tensorflow/result/pred.txt'\n",
        "# What's more, in order to interact with the variables in 'main.py', we need to paste the codes in a Colab cell. Thus rather than updating the parameters with the below line, we do the following steps.\n",
        "\n",
        "# !python main.py --valid_result_path=/content/ethnicity-tensorflow/result/valid --pred_result_path=/content/ethnicity-tensorflow/result/pred.txt ......"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D-O6uyJ7L5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow\n",
        "# import tensorflow.python\n",
        "# import tensorflow.python.framework"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGP_nAFE7VOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.python.client import device_lib\n",
        "# local_device_protos = device_lib.list_local_devices()\n",
        "# print([x.name for x in local_device_protos if x.device_type == 'GPU'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGEi4C5D7aGk",
        "colab_type": "code",
        "outputId": "048b854c-25dc-4a8d-e82b-3e2916337de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pprint\n",
        "\n",
        "from time import gmtime, strftime\n",
        "from dataset import get_data, experiment, get_char2vec\n",
        "from model import RNN\n",
        "\n",
        "\n",
        "# flags = tf.app.flags # Pass the parameters\n",
        "\n",
        "# # Default parameters\n",
        "# flags.DEFINE_integer(\"train_epoch\", 3000, \"Epoch to train\")\n",
        "# flags.DEFINE_integer(\"dim_unigram\", 82, \"Dimension of input, 42 or 82\")\n",
        "# flags.DEFINE_integer(\"dim_bigram\", 1876, \"Dimension of input, 925 or 1876\")\n",
        "# flags.DEFINE_integer(\"dim_trigram\", 14767, \"Dimension of input, 8573 or 14767\")\n",
        "# flags.DEFINE_integer(\"dim_output\", 127, \"Dimension of output, 95 or 127\") # Number of nationalities\n",
        "# flags.DEFINE_integer(\"max_time_step\", 60, \"Maximum time step of RNN\")\n",
        "# flags.DEFINE_integer(\"min_grad\", -5, \"Minimum gradient to clip\")\n",
        "# flags.DEFINE_integer(\"max_grad\", 5, \"Maximum gradient to clip\")\n",
        "# flags.DEFINE_integer(\"batch_size\", 300, \"Size of batch\")\n",
        "# flags.DEFINE_integer(\"ngram\", 3, \"Ngram feature when ensemble = False.\")\n",
        "# flags.DEFINE_float(\"decay_rate\", 0.99, \"Decay rate of learning rate\")\n",
        "# flags.DEFINE_float(\"decay_step\", 100, \"Decay step of learning rate\")\n",
        "\n",
        "# # Validation hyper parameters\n",
        "# flags.DEFINE_integer(\"valid_iteration\", 250, \"Number of validation iteration.\")\n",
        "# flags.DEFINE_integer(\"dim_rnn_cell\", 200, \"Dimension of RNN cell\") # (200, 1) or (200, 1 * dim_embed_gram)\n",
        "# flags.DEFINE_integer(\"dim_rnn_cell_min\", 200, \"Minimum dimension of RNN cell\")\n",
        "# flags.DEFINE_integer(\"dim_rnn_cell_max\", 399, \"Maximum dimension of RNN cell\")\n",
        "# flags.DEFINE_integer(\"dim_hidden\", 200, \"Dimension of hidden layer\") # (200, 1)\n",
        "# flags.DEFINE_integer(\"dim_hidden_min\", 200, \"Minimum dimension of hidden layer\")\n",
        "# flags.DEFINE_integer(\"dim_hidden_max\", 399, \"Maximum dimension of hidden layer\")\n",
        "# flags.DEFINE_integer(\"dim_embed_unigram\", 30, \"Dimension of character embedding\") # Why increase from uni to tri?\n",
        "# flags.DEFINE_integer(\"dim_embed_unigram_min\", 10, \"Minimum dimension of character embedding\")\n",
        "# flags.DEFINE_integer(\"dim_embed_unigram_max\", 100, \"Maximum dimension of character embedding\")\n",
        "# flags.DEFINE_integer(\"dim_embed_bigram\", 100, \"Dimension of character embedding\")\n",
        "# flags.DEFINE_integer(\"dim_embed_bigram_min\", 30, \"Minimum dimension of character embedding\")\n",
        "# flags.DEFINE_integer(\"dim_embed_bigram_max\", 200, \"Maximum dimension of character embedding\")\n",
        "# flags.DEFINE_integer(\"dim_embed_trigram\", 130, \"Dimension of character embedding\")\n",
        "# flags.DEFINE_integer(\"dim_embed_trigram_min\", 30, \"Minimum dimension of character embedding\")\n",
        "# flags.DEFINE_integer(\"dim_embed_trigram_max\", 320, \"Maximum dimension of character embedding\")\n",
        "# flags.DEFINE_integer(\"lstm_layer\", 1, \"Layer number of RNN \")\n",
        "# flags.DEFINE_integer(\"lstm_layer_min\", 1, \"Mimimum layer number of RNN \")\n",
        "# flags.DEFINE_integer(\"lstm_layer_max\", 1, \"Maximum layer number of RNN \")\n",
        "# flags.DEFINE_float(\"lstm_dropout\", 0.5, \"Dropout of RNN cell\")\n",
        "# flags.DEFINE_float(\"lstm_dropout_min\", 0.3, \"Minumum dropout of RNN cell\")\n",
        "# flags.DEFINE_float(\"lstm_dropout_max\", 0.8, \"Maximum dropout of RNN cell\")\n",
        "# flags.DEFINE_float(\"hidden_dropout\", 0.5, \"Dropout rate of hidden layer\")\n",
        "# flags.DEFINE_float(\"hidden_dropout_min\", 0.3, \"Minimum dropout rate of hidden layer\")\n",
        "# flags.DEFINE_float(\"hidden_dropout_max\", 0.8, \"Maximum dropout rate of hidden layer\")\n",
        "# flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate of the optimzier\")\n",
        "# flags.DEFINE_float(\"learning_rate_min\", 5e-3, \"Minimum learning rate of the optimzier\")\n",
        "# flags.DEFINE_float(\"learning_rate_max\", 5e-2, \"Maximum learning rate of the optimzier\")\n",
        "\n",
        "# # Model settings\n",
        "# flags.DEFINE_boolean(\"default_params\", True, \"True to use default params\")\n",
        "# flags.DEFINE_boolean(\"ensemble\", True, \"True to use ensemble ngram\")\n",
        "# flags.DEFINE_boolean(\"embed\", True, \"True to use embedding table\")\n",
        "# flags.DEFINE_boolean(\"embed_trainable\", False, \"True to use embedding table\")\n",
        "# flags.DEFINE_boolean(\"ethnicity\", False, \"True to test on ethnicity\")\n",
        "# flags.DEFINE_boolean(\"is_train\", True, \"True for training, False for testing\")\n",
        "# flags.DEFINE_boolean(\"is_valid\", True, \"True for validation, False for testing\")\n",
        "# flags.DEFINE_boolean(\"continue_train\", False, \"True to continue training from saved checkpoint. False for restarting.\")\n",
        "# flags.DEFINE_boolean(\"save\", False, \"True to save\")\n",
        "# flags.DEFINE_string(\"model_name\", \"default\", \"Model name, auto saved as YMDHMS\")\n",
        "# flags.DEFINE_string(\"checkpoint_dir\", \"./checkpoint/\", \"Directory name to save the checkpoints [checkpoint]\")\n",
        "# flags.DEFINE_string(\"data_dir\", \"data/raw\", \"Directory name of input data\")\n",
        "# flags.DEFINE_string(\"valid_result_path\", \"result/validation\", \"Validation result save path\")\n",
        "# flags.DEFINE_string(\"pred_result_path\", \"result/pred.txt\", \"Prediction result save path\")\n",
        "# flags.DEFINE_string(\"detail_result_path\", \"result/detail.txt\", \"Prediction result save path\")\n",
        "\n",
        "# FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "def sample_parameters(params):\n",
        "    combination = [\n",
        "            params['dim_hidden'],\n",
        "            params['dim_rnn_cell'],\n",
        "            params['learning_rate'],\n",
        "            params['lstm_dropout'],\n",
        "            params['lstm_layer'],\n",
        "            params['hidden_dropout'],\n",
        "            params['dim_embed_unigram'],\n",
        "            params['dim_embed_bigram'],\n",
        "            params['dim_embed_trigram']\n",
        "    ]\n",
        "\n",
        "    if not params['default_params']: # If not set too use default params\n",
        "        combination[0] = params['dim_hidden'] = int(np.random.uniform(\n",
        "                params['dim_hidden_min'],\n",
        "                params['dim_hidden_max']) // 50) * 50 \n",
        "        combination[1] = params['dim_rnn_cell'] = int(np.random.uniform(\n",
        "                params['dim_rnn_cell_min'],\n",
        "                params['dim_rnn_cell_max']) // 50) * 50\n",
        "        combination[2] = params['learning_rate'] = float('{0:.5f}'.format(np.random.uniform( # We could improve with log sampling\n",
        "                params['learning_rate_min'],\n",
        "                params['learning_rate_max'])))\n",
        "        combination[3] = params['lstm_dropout'] = float('{0:.5f}'.format(np.random.uniform( # 5 after the decimal point\n",
        "                params['lstm_dropout_min'],\n",
        "                params['lstm_dropout_max'])))\n",
        "        combination[4] = params['lstm_layer'] = int(np.random.uniform(\n",
        "                params['lstm_layer_min'],\n",
        "                params['lstm_layer_max']))\n",
        "        combination[5] = params['hidden_dropout'] = float('{0:.5f}'.format(np.random.uniform(\n",
        "                params['hidden_dropout_min'],\n",
        "                params['hidden_dropout_max'])))\n",
        "        combination[6] = params['dim_embed_unigram'] = int(np.random.uniform(\n",
        "                params['dim_embed_unigram_min'],\n",
        "                params['dim_embed_unigram_max']) // 10) * 10\n",
        "        combination[7] = params['dim_embed_bigram'] = int(np.random.uniform(\n",
        "                params['dim_embed_bigram_min'],\n",
        "                params['dim_embed_bigram_max']) // 10) * 10\n",
        "        combination[8] = params['dim_embed_trigram'] = int(np.random.uniform(\n",
        "                params['dim_embed_trigram_min'],\n",
        "                params['dim_embed_trigram_max']) // 10) * 10\n",
        "\n",
        "    return params, combination\n",
        "\n",
        "\n",
        "# def main(_):\n",
        "    # Save default params and set scope"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtC0QS9ZPH8p",
        "colab_type": "code",
        "outputId": "c268c306-b9d1-4200-85fd-20a05c54d95f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "saved_params = { # Manually input the flags above so that we could interact with the variables below\n",
        "    'train_epoch' : 3000,\n",
        "    \"dim_unigram\" : 82,\n",
        "    \"dim_bigram\" : 1876,\n",
        "    \"dim_trigram\" : 14767,\n",
        "    \"dim_output\" : 18,\n",
        "    \"max_time_step\" : 50,\n",
        "    \"min_grad\" : -5,\n",
        "    \"max_grad\" : 5,\n",
        "    \"batch_size\" : 1000,\n",
        "    \"ngram\" : 3,\n",
        "    \"decay_rate\" : 0.99,\n",
        "    \"decay_step\" : 100,\n",
        "    \"valid_iteration\" : 250,\n",
        "    \"dim_rnn_cell\" : 200,\n",
        "    \"dim_rnn_cell_min\" : 200,\n",
        "    \"dim_rnn_cell_max\" : 399,\n",
        "    \"dim_hidden\" : 200,\n",
        "    \"dim_hidden_min\" : 200,\n",
        "    \"dim_hidden_max\" : 399,\n",
        "    \"dim_embed_unigram\" : 30,\n",
        "    \"dim_embed_unigram_min\" : 10,\n",
        "    \"dim_embed_unigram_max\" : 100,\n",
        "    \"dim_embed_bigram\" : 100,\n",
        "    \"dim_embed_bigram_min\" : 30,\n",
        "    \"dim_embed_bigram_max\" : 200,\n",
        "    \"dim_embed_trigram\" : 130,\n",
        "    \"dim_embed_trigram_min\" : 30,\n",
        "    \"dim_embed_trigram_max\" : 320,\n",
        "    \"lstm_layer\" : 1,\n",
        "    \"lstm_layer_min\" : 1,\n",
        "    \"lstm_layer_max\" : 1,\n",
        "    \"lstm_dropout\" : 0.5,\n",
        "    \"lstm_dropout_min\" : 0.3,\n",
        "    \"lstm_dropout_max\" : 0.8,\n",
        "    \"hidden_dropout\" : 0.5,\n",
        "    \"hidden_dropout_min\" : 0.3,\n",
        "    \"hidden_dropout_max\" : 0.8,\n",
        "    \"learning_rate\" : 0.0035,\n",
        "    \"learning_rate_min\" : 5e-3,\n",
        "    \"learning_rate_max\" : 5e-2,\n",
        "    \"default_params\" : True,\n",
        "    \"ensemble\" : True,\n",
        "    \"embed\" : True,\n",
        "    \"embed_trainable\" : False,\n",
        "    \"ethnicity\" : False,\n",
        "    \"is_train\" : True,\n",
        "    \"is_valid\" : True,\n",
        "    \"continue_train\" : False,\n",
        "    \"save\" : False,\n",
        "    \"model_name\" : \"default\",\n",
        "    \"checkpoint_dir\" : \"./checkpoint/\",\n",
        "    \"data_dir\" : \"./data/test\",\n",
        "    \"valid_result_path\" : \"/content/CS230/result/validation\",\n",
        "    \"pred_result_path\" : \"/content/CS230/result/pred.txt\",\n",
        "    \"detail_result_path\" : \"/content/CS230/result/detail.txt\"\n",
        "}\n",
        "# print(saved_params)\n",
        "\n",
        "if saved_params['ensemble']: # uni + bi + tri\n",
        "    model_name = 'ensemble'\n",
        "elif saved_params['ngram'] == 1:\n",
        "    model_name = 'unigram'\n",
        "elif saved_params['ngram'] == 2:\n",
        "    model_name = 'bigram'\n",
        "elif saved_params['ngram'] == 3:\n",
        "    model_name = 'trigram'\n",
        "else:\n",
        "    assert True, 'Not supported ngram %d'% saved_params['ngram']\n",
        "model_name += '_embedding' if saved_params['embed'] else '_no_embedding' \n",
        "saved_params['model_name'] = '%s' % model_name\n",
        "saved_params['checkpoint_dir'] += model_name\n",
        "pprint.PrettyPrinter().pprint(saved_params)\n",
        "saved_dataset = get_data(saved_params) # Input the passing parameters; Return train_set, valid_set, test_set, dictionary == [idx2unigram, unigram2idx, idx2country, country2ethnicity, idx2bigram, idx2trigram]\n",
        "\n",
        "validation_writer = open(saved_params['valid_result_path'], 'a') # 'a' => Write in a new file if not existing\n",
        "validation_writer.write(model_name + \"\\n\")\n",
        "validation_writer.write(\"[dim_hidden, dim_rnn_cell, learning_rate, lstm_dropout, lstm_layer, hidden_dropout, dim_embed]\\n\")\n",
        "validation_writer.write(\"combination\\ttop1\\ttop5\\tepoch\\n\")\n",
        "\n",
        "# Run the model\n",
        "for _ in range(saved_params['valid_iteration']):\n",
        "    # Sample parameter sets\n",
        "    params, combination = sample_parameters(saved_params.copy()) # If not default parameters, then update with initialization\n",
        "    dataset = saved_dataset[:]\n",
        "    \n",
        "    # Initialize embeddings\n",
        "    uni_init = get_char2vec(dataset[0][0][:], params['dim_embed_unigram'], dataset[3][0]) # Return initializer\n",
        "    bi_init = get_char2vec(dataset[0][1][:], params['dim_embed_bigram'], dataset[3][4]) # The first [] is the outermost dimension == train_set or dictionary; [3][i] gives the outermost dimension in dictionary\n",
        "    tri_init = get_char2vec(dataset[0][2][:], params['dim_embed_trigram'], dataset[3][5]) # Easy to understand with get_data()\n",
        "    \n",
        "    print(model_name, 'Parameter sets: ', end='')\n",
        "    pprint.PrettyPrinter().pprint(combination)\n",
        "    \n",
        "    rnn_model = RNN(params, [uni_init, bi_init, tri_init])\n",
        "    top1, top5, ep = experiment(rnn_model, dataset, params)\n",
        "    \n",
        "    validation_writer.write(str(combination) + '\\t')\n",
        "    validation_writer.write(str(top1) + '\\t' + str(top5) + '\\tEp:' + str(ep) + '\\n')\n",
        "\n",
        "validation_writer.close()\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 1000,\n",
            " 'checkpoint_dir': './checkpoint/ensemble_embedding',\n",
            " 'continue_train': False,\n",
            " 'data_dir': './data/test',\n",
            " 'decay_rate': 0.99,\n",
            " 'decay_step': 100,\n",
            " 'default_params': True,\n",
            " 'detail_result_path': '/content/CS230/result/detail.txt',\n",
            " 'dim_bigram': 1876,\n",
            " 'dim_embed_bigram': 100,\n",
            " 'dim_embed_bigram_max': 200,\n",
            " 'dim_embed_bigram_min': 30,\n",
            " 'dim_embed_trigram': 130,\n",
            " 'dim_embed_trigram_max': 320,\n",
            " 'dim_embed_trigram_min': 30,\n",
            " 'dim_embed_unigram': 30,\n",
            " 'dim_embed_unigram_max': 100,\n",
            " 'dim_embed_unigram_min': 10,\n",
            " 'dim_hidden': 200,\n",
            " 'dim_hidden_max': 399,\n",
            " 'dim_hidden_min': 200,\n",
            " 'dim_output': 18,\n",
            " 'dim_rnn_cell': 200,\n",
            " 'dim_rnn_cell_max': 399,\n",
            " 'dim_rnn_cell_min': 200,\n",
            " 'dim_trigram': 14767,\n",
            " 'dim_unigram': 82,\n",
            " 'embed': True,\n",
            " 'embed_trainable': False,\n",
            " 'ensemble': True,\n",
            " 'ethnicity': False,\n",
            " 'hidden_dropout': 0.5,\n",
            " 'hidden_dropout_max': 0.8,\n",
            " 'hidden_dropout_min': 0.3,\n",
            " 'is_train': True,\n",
            " 'is_valid': True,\n",
            " 'learning_rate': 0.0035,\n",
            " 'learning_rate_max': 0.05,\n",
            " 'learning_rate_min': 0.005,\n",
            " 'lstm_dropout': 0.5,\n",
            " 'lstm_dropout_max': 0.8,\n",
            " 'lstm_dropout_min': 0.3,\n",
            " 'lstm_layer': 1,\n",
            " 'lstm_layer_max': 1,\n",
            " 'lstm_layer_min': 1,\n",
            " 'max_grad': 5,\n",
            " 'max_time_step': 50,\n",
            " 'min_grad': -5,\n",
            " 'model_name': 'ensemble_embedding',\n",
            " 'ngram': 3,\n",
            " 'pred_result_path': '/content/CS230/result/pred.txt',\n",
            " 'save': False,\n",
            " 'train_epoch': 3000,\n",
            " 'valid_iteration': 250,\n",
            " 'valid_result_path': '/content/CS230/result/validation'}\n",
            "reading 0_unigram_to_idx.txt of length 71\n",
            "reading 1_bigram_to_idx.txt of length 1055\n",
            "reading 2_trigram_to_idx.txt of length 11327\n",
            "reading country_to_idx.txt of length 18\n",
            "reading data_test_test of length 38346\n",
            "reading data_test_train of length 115042\n",
            "reading data_test_valid of length 38348\n",
            "total data length: 115042 38348 38346\n",
            "shape of data: (5, 115042) (5, 38348) (5, 38346)\n",
            "name max length: 42\n",
            "[39, 52, 45, 43, 50, 39, 0, 46, 39, 45, 47, 39, 57]\n",
            "[304, 682, 470, 418, 615, 285, 33, 496, 297, 474, 526, 309]\n",
            "[1588, 6779, 3921, 3215, 5777, 1212, 269, 4219, 1408, 3971, 4620]\n",
            "13 6\n",
            "shape of data: (5, 115042) (5, 38348) (5, 38346)\n",
            "preprocessing done\n",
            "\n",
            "ensemble_embedding Parameter sets: [200, 200, 0.0035, 0.5, 1, 0.5, 30, 100, 130]\n",
            "## Building an RNN model\n",
            "Tensor(\"Unigram/Unigram/embedding_lookup:0\", shape=(?, 50, 30), dtype=float32)\n",
            "Tensor(\"Bigram/Bigram/embedding_lookup:0\", shape=(?, 50, 100), dtype=float32)\n",
            "Tensor(\"Trigram/Trigram/embedding_lookup:0\", shape=(?, 50, 130), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model variables ['Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Hidden1/Weights:0', 'Hidden1/Biases:0', 'Output/Weights:0', 'Output/Biases:0']\n",
            "## Training\n",
            "Percent: [#####               ] 22.60%  tr loss: 1.761, acc1: 0.674, acc5: 0.959"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAeiEynEHJbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = saved_dataset\n",
        "train_set = dataset[0]\n",
        "valid_set = dataset[1]\n",
        "test_set = dataset[2]\n",
        "[idx2unigram, unigram2idx, idx2country, country2ethnicity, idx2bigram, idx2trigram] = dataset[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oTSUd6UOyNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test\n",
        "# np.shape(test_set[1][0])\n",
        "# test_set[1][200]\n",
        "# len(dataset[3][5])\n",
        "# idx2unigram[0]\n",
        "# len(train_set)\n",
        "# len(train_set[:][3]) # Why is it still 10754??? \n",
        "# np.shape(train_set[2][101])\n",
        "# idx2bigram\n",
        "train_set[2][100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY-RNjZ76Iku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}