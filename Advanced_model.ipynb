{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY0a_2V7VouA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the environment 1\n",
        "!pip install gensim \n",
        "\n",
        "# Set the environment 2\n",
        "!pip install tensorflow==1.0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1MtQcURV_Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone RNN resources \n",
        "!git clone  https://github.com/weirenorweiren/CS230.git\n",
        "\n",
        "# Change the working directory to e-t; not to comment directly after '%cd xxx' which could cause error\n",
        "%cd CS230\n",
        "\n",
        "# Make a directory called result in the current working directory\n",
        "!mkdir result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rnS5hcWWpS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pprint\n",
        "\n",
        "from time import gmtime, strftime\n",
        "from dataset_ad import get_data, experiment, get_char2vec\n",
        "from model_ad import RNN\n",
        "\n",
        "def sample_parameters(params):\n",
        "    combination = [\n",
        "            params['dim_hidden'],\n",
        "            params['dim_rnn_cell'],\n",
        "            params['learning_rate'],\n",
        "            params['lstm_dropout'],\n",
        "            params['lstm_layer'],\n",
        "            params['hidden_dropout'],\n",
        "            params['dim_embed_unigram'],\n",
        "            params['dim_embed_bigram'],\n",
        "            params['dim_embed_trigram'],\n",
        "\n",
        "            params['dim_embed_fourgram']\n",
        "    ]\n",
        "\n",
        "    if not params['default_params']: # If not set too use default params\n",
        "        combination[0] = params['dim_hidden'] = int(np.random.uniform(\n",
        "                params['dim_hidden_min'],\n",
        "                params['dim_hidden_max']) // 50) * 50 \n",
        "        combination[1] = params['dim_rnn_cell'] = int(np.random.uniform(\n",
        "                params['dim_rnn_cell_min'],\n",
        "                params['dim_rnn_cell_max']) // 50) * 50\n",
        "        combination[2] = params['learning_rate'] = float('{0:.5f}'.format(np.random.uniform( # We could improve with log sampling\n",
        "                params['learning_rate_min'],\n",
        "                params['learning_rate_max'])))\n",
        "        combination[3] = params['lstm_dropout'] = float('{0:.5f}'.format(np.random.uniform( # 5 after the decimal point\n",
        "                params['lstm_dropout_min'],\n",
        "                params['lstm_dropout_max'])))\n",
        "        combination[4] = params['lstm_layer'] = int(np.random.uniform(\n",
        "                params['lstm_layer_min'],\n",
        "                params['lstm_layer_max']))\n",
        "        combination[5] = params['hidden_dropout'] = float('{0:.5f}'.format(np.random.uniform(\n",
        "                params['hidden_dropout_min'],\n",
        "                params['hidden_dropout_max'])))\n",
        "        combination[6] = params['dim_embed_unigram'] = int(np.random.uniform(\n",
        "                params['dim_embed_unigram_min'],\n",
        "                params['dim_embed_unigram_max']) // 10) * 10\n",
        "        combination[7] = params['dim_embed_bigram'] = int(np.random.uniform(\n",
        "                params['dim_embed_bigram_min'],\n",
        "                params['dim_embed_bigram_max']) // 10) * 10\n",
        "        combination[8] = params['dim_embed_trigram'] = int(np.random.uniform(\n",
        "                params['dim_embed_trigram_min'],\n",
        "                params['dim_embed_trigram_max']) // 10) * 10\n",
        "\n",
        "        combination[9] = params['dim_embed_fourgram'] = int(np.random.uniform(\n",
        "                params['dim_embed_fourgram_min'],\n",
        "                params['dim_embed_fourgram_max']) // 10) * 10\n",
        "\n",
        "    return params, combination\n",
        "\n",
        "# !!! Always to update the parameters here !!!\n",
        "saved_params = { # Manually input the flags above so that we could interact with the variables below\n",
        "    'train_epoch' : 3000,\n",
        "    \"dim_unigram\" : 82,\n",
        "    \"dim_bigram\" : 1876,\n",
        "    \"dim_trigram\" : 14767,\n",
        "\n",
        "    \"dim_fourgram\" : 50596,\n",
        "\n",
        "    \"dim_output\" : 127,\n",
        "    \"max_time_step\" : 50,\n",
        "    \"min_grad\" : -5,\n",
        "    \"max_grad\" : 5,\n",
        "    \"batch_size\" : 1000,\n",
        "\n",
        "    \"ngram\" : 4,\n",
        "    \n",
        "    \"decay_rate\" : 0.99,\n",
        "    \"decay_step\" : 100,\n",
        "\n",
        "    \"valid_iteration\" : 250,\n",
        "\n",
        "    \"dim_rnn_cell\" : 200,\n",
        "    \"dim_rnn_cell_min\" : 200,\n",
        "    \"dim_rnn_cell_max\" : 399,\n",
        "\n",
        "    \"dim_hidden\" : 200,\n",
        "    \"dim_hidden_min\" : 200,\n",
        "    \"dim_hidden_max\" : 399,\n",
        "\n",
        "    \"dim_embed_unigram\" : 30,\n",
        "    \"dim_embed_unigram_min\" : 10,\n",
        "    \"dim_embed_unigram_max\" : 100,\n",
        "\n",
        "    \"dim_embed_bigram\" : 100,\n",
        "    \"dim_embed_bigram_min\" : 30,\n",
        "    \"dim_embed_bigram_max\" : 200,\n",
        "\n",
        "    \"dim_embed_trigram\" : 130,\n",
        "    \"dim_embed_trigram_min\" : 30,\n",
        "    \"dim_embed_trigram_max\" : 320,\n",
        "\n",
        "    \"dim_embed_fourgram\" : 200,\n",
        "    \"dim_embed_fourgram_min\" : 30,\n",
        "    \"dim_embed_fourgram_max\" : 320,\n",
        "\n",
        "    \"lstm_layer\" : 1,\n",
        "    \"lstm_layer_min\" : 1,\n",
        "    \"lstm_layer_max\" : 1,\n",
        "\n",
        "    \"lstm_dropout\" : 0.5,\n",
        "    \"lstm_dropout_min\" : 0.3,\n",
        "    \"lstm_dropout_max\" : 0.8,\n",
        "\n",
        "    \"hidden_dropout\" : 0.5,\n",
        "    \"hidden_dropout_min\" : 0.3,\n",
        "    \"hidden_dropout_max\" : 0.8,\n",
        "\n",
        "    \"learning_rate\" : 0.0035,\n",
        "    \"learning_rate_min\" : 5e-3,\n",
        "    \"learning_rate_max\" : 5e-2,\n",
        "\n",
        "    \"default_params\" : True,\n",
        "    \"ensemble\" : True,\n",
        "\n",
        "    \"embed\" : True,\n",
        "    \"embed_trainable\" : True,\n",
        "    \n",
        "    \"ethnicity\" : False,\n",
        "    \"is_train\" : True,\n",
        "    \"is_valid\" : True,\n",
        "    \"continue_train\" : False,\n",
        "    \"save\" : False,\n",
        "    \"model_name\" : \"default\",\n",
        "    \"checkpoint_dir\" : \"./checkpoint/\",\n",
        "\n",
        "    \"data_dir\" : \"./data/ad\",\n",
        "    \n",
        "    \"valid_result_path\" : \"/content/CS230/result/validation.txt\",\n",
        "    \"pred_result_path\" : \"/content/CS230/result/pred.txt\",\n",
        "    \"detail_result_path\" : \"/content/CS230/result/detail.txt\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmV7jDdsW79q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c205fc56-70ab-41ac-e31d-d39486e2e53b"
      },
      "source": [
        "if saved_params['ensemble']: # uni + bi + tri +four\n",
        "    model_name = 'ensemble'\n",
        "elif saved_params['ngram'] == 1:\n",
        "    model_name = 'unigram'\n",
        "elif saved_params['ngram'] == 2:\n",
        "    model_name = 'bigram'\n",
        "elif saved_params['ngram'] == 3:\n",
        "    model_name = 'trigram'\n",
        "\n",
        "elif saved_params['ngram'] == 4:\n",
        "    model_name = 'fourgram'\n",
        "\n",
        "else:\n",
        "    assert False, 'Not supported ngram %d'% saved_params['ngram'] # ** Origin value is True\n",
        "model_name += '_embedding' if saved_params['embed'] else '_no_embedding' \n",
        "saved_params['model_name'] = '%s' % model_name\n",
        "saved_params['checkpoint_dir'] += model_name\n",
        "pprint.PrettyPrinter().pprint(saved_params)\n",
        "saved_dataset = get_data(saved_params) # Input the passing parameters; Return train_set, valid_set, test_set, dictionary == [idx2unigram, unigram2idx, idx2country, country2ethnicity, idx2bigram, idx2trigram]\n",
        "\n",
        "validation_writer = open(saved_params['valid_result_path'], 'a') # Open a file and add from the last ending; write in a new file if not existing\n",
        "validation_writer.write(model_name + \"\\n\")\n",
        "validation_writer.write(\"[dim_hidden, dim_rnn_cell, learning_rate, lstm_dropout, lstm_layer, hidden_dropout, dim_embed]\\n\")\n",
        "validation_writer.write(\"combination\\ttop1\\ttop5\\tepoch\\n\") # \\t => tab"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 1000,\n",
            " 'checkpoint_dir': './checkpoint/ensemble_embedding',\n",
            " 'continue_train': False,\n",
            " 'data_dir': './data/ad',\n",
            " 'decay_rate': 0.99,\n",
            " 'decay_step': 100,\n",
            " 'default_params': True,\n",
            " 'detail_result_path': '/content/CS230/result/detail.txt',\n",
            " 'dim_bigram': 1876,\n",
            " 'dim_embed_bigram': 100,\n",
            " 'dim_embed_bigram_max': 200,\n",
            " 'dim_embed_bigram_min': 30,\n",
            " 'dim_embed_fourgram': 200,\n",
            " 'dim_embed_fourgram_max': 320,\n",
            " 'dim_embed_fourgram_min': 30,\n",
            " 'dim_embed_trigram': 130,\n",
            " 'dim_embed_trigram_max': 320,\n",
            " 'dim_embed_trigram_min': 30,\n",
            " 'dim_embed_unigram': 30,\n",
            " 'dim_embed_unigram_max': 100,\n",
            " 'dim_embed_unigram_min': 10,\n",
            " 'dim_fourgram': 50596,\n",
            " 'dim_hidden': 200,\n",
            " 'dim_hidden_max': 399,\n",
            " 'dim_hidden_min': 200,\n",
            " 'dim_output': 127,\n",
            " 'dim_rnn_cell': 200,\n",
            " 'dim_rnn_cell_max': 399,\n",
            " 'dim_rnn_cell_min': 200,\n",
            " 'dim_trigram': 14767,\n",
            " 'dim_unigram': 82,\n",
            " 'embed': True,\n",
            " 'embed_trainable': True,\n",
            " 'ensemble': True,\n",
            " 'ethnicity': False,\n",
            " 'hidden_dropout': 0.5,\n",
            " 'hidden_dropout_max': 0.8,\n",
            " 'hidden_dropout_min': 0.3,\n",
            " 'is_train': True,\n",
            " 'is_valid': True,\n",
            " 'learning_rate': 0.0035,\n",
            " 'learning_rate_max': 0.05,\n",
            " 'learning_rate_min': 0.005,\n",
            " 'lstm_dropout': 0.5,\n",
            " 'lstm_dropout_max': 0.8,\n",
            " 'lstm_dropout_min': 0.3,\n",
            " 'lstm_layer': 1,\n",
            " 'lstm_layer_max': 1,\n",
            " 'lstm_layer_min': 1,\n",
            " 'max_grad': 5,\n",
            " 'max_time_step': 50,\n",
            " 'min_grad': -5,\n",
            " 'model_name': 'ensemble_embedding',\n",
            " 'ngram': 4,\n",
            " 'pred_result_path': '/content/CS230/result/pred.txt',\n",
            " 'save': False,\n",
            " 'train_epoch': 3000,\n",
            " 'valid_iteration': 250,\n",
            " 'valid_result_path': '/content/CS230/result/validation.txt'}\n",
            "ignoring file .DS_Store\n",
            "reading .DS_Store of length 0\n",
            "reading 0_unigram_to_idx.txt of length 82\n",
            "reading 1_bigram_to_idx.txt of length 1876\n",
            "reading 2_trigram_to_idx.txt of length 14767\n",
            "reading 3_fourgram_to_idx.txt of length 50596\n",
            "reading country_to_idx.txt of length 127\n",
            "reading data_ad_test of length 3543\n",
            "reading data_ad_train of length 10633\n",
            "reading data_ad_valid of length 3545\n",
            "total data length: 10633 3545 3543\n",
            "shape of data: (6, 10633) (6, 3545) (6, 3543)\n",
            "name max length: 47\n",
            "[11, 43, 48, 44, 48, 0, 19, 8, 29, 25, 21, 16, 10]\n",
            "[290, 1400, 1529, 1421, 1515, 16, 616, 152, 1023, 848, 708, 485]\n",
            "[2592, 11821, 12836, 11953, 12712, 337, 5224, 1504, 9049, 7419, 6126]\n",
            "[9961, 40449, 44322, 40891, 43939, 1839, 18296, 6496, 31204, 25809]\n",
            "13 124\n",
            "shape of data: (6, 10633) (6, 3545) (6, 3543)\n",
            "preprocessing done\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoGeSb1UahdG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6aa2fa8e-eab6-4d7b-8b50-8dbd17a21b38"
      },
      "source": [
        "# Run the model\n",
        "for _ in range(saved_params['valid_iteration']): # ??? != valid_epoch\n",
        "    # Sample parameter sets\n",
        "    params, combination = sample_parameters(saved_params.copy()) # If not default parameters, then update with initialization; return input dictionary and a combination LIST\n",
        "    dataset = saved_dataset[:] # Copy the content into dataset; if not, we would link the two variable that can be a problem\n",
        "    \n",
        "    # Initialize embeddings\n",
        "    uni_init = get_char2vec(dataset[0][0][:], params['dim_embed_unigram'], dataset[3][0]) # Return initializer\n",
        "    bi_init = get_char2vec(dataset[0][1][:], params['dim_embed_bigram'], dataset[3][4]) # The first [] is the outermost dimension == train_set or dictionary; [3][i] gives the outermost dimension in dictionary\n",
        "    tri_init = get_char2vec(dataset[0][2][:], params['dim_embed_trigram'], dataset[3][5]) # Easy to understand with get_data()\n",
        "\n",
        "    four_init = get_char2vec(dataset[0][3][:], params['dim_embed_fourgram'], dataset[3][6])\n",
        "    \n",
        "    print(model_name, 'Parameter sets: ', end='')\n",
        "    pprint.PrettyPrinter().pprint(combination)\n",
        "    \n",
        "    rnn_model = RNN(params, [uni_init, bi_init, tri_init, four_init])\n",
        "    top1, top5, ep = experiment(rnn_model, dataset, params) # With train_iterations; return max_top1, max_top5, max_top1_epoch\n",
        "    \n",
        "    validation_writer.write(str(combination) + '\\t')\n",
        "    validation_writer.write(str(top1) + '\\t' + str(top5) + '\\tEp:' + str(ep) + '\\n')\n",
        "\n",
        "validation_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ensemble_embedding Parameter sets: [200, 200, 0.0035, 0.5, 1, 0.5, 30, 100, 130, 200]\n",
            "## Building an RNN model\n",
            "Tensor(\"Unigram/Unigram/embedding_lookup:0\", shape=(?, 50, 30), dtype=float32)\n",
            "Tensor(\"Bigram/Bigram/embedding_lookup:0\", shape=(?, 50, 100), dtype=float32)\n",
            "Tensor(\"Trigram/Trigram/embedding_lookup:0\", shape=(?, 50, 130), dtype=float32)\n",
            "Tensor(\"Fourgram/Fourgram/embedding_lookup:0\", shape=(?, 50, 200), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model variables ['Unigram/embed:0', 'Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Bigram/embed:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Trigram/embed:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Fourgram/embed:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Fourgram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Hidden1/Weights:0', 'Hidden1/Biases:0', 'Output/Weights:0', 'Output/Biases:0']\n",
            "## Training\n",
            "Percent: [####################] 100.00% Finished. tr loss: 12.015, acc1: 0.017, acc5: 0.077\n",
            "Training loss: 60.242, acc1: 0.019, acc5: 0.067, ep: 0\n",
            "\n",
            "Validation loss: 4.782, acc1: 0.025, acc5: 0.218, ep: 0\n",
            "Testing loss: 4.792, acc1: 0.026, acc5: 0.217\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 6.287, acc1: 0.035, acc5: 0.161\n",
            "Training loss: 8.448, acc1: 0.028, acc5: 0.115, ep: 1\n",
            "\n",
            "Validation loss: 4.750, acc1: 0.062, acc5: 0.363, ep: 1\n",
            "Testing loss: 4.756, acc1: 0.064, acc5: 0.351\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 5.184, acc1: 0.044, acc5: 0.215\n",
            "Training loss: 5.574, acc1: 0.039, acc5: 0.184, ep: 2\n",
            "\n",
            "Validation loss: 4.695, acc1: 0.102, acc5: 0.367, ep: 2\n",
            "Testing loss: 4.700, acc1: 0.084, acc5: 0.352\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 4.871, acc1: 0.079, acc5: 0.295\n",
            "Training loss: 4.968, acc1: 0.073, acc5: 0.256, ep: 3\n",
            "\n",
            "Validation loss: 4.595, acc1: 0.121, acc5: 0.373, ep: 3\n",
            "Testing loss: 4.612, acc1: 0.107, acc5: 0.361\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 4.660, acc1: 0.090, acc5: 0.330\n",
            "Training loss: 4.763, acc1: 0.086, acc5: 0.287, ep: 4\n",
            "\n",
            "Validation loss: 4.523, acc1: 0.128, acc5: 0.375, ep: 4\n",
            "Testing loss: 4.537, acc1: 0.117, acc5: 0.365\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 4.490, acc1: 0.133, acc5: 0.340\n",
            "Training loss: 4.648, acc1: 0.105, acc5: 0.298, ep: 5\n",
            "\n",
            "Validation loss: 4.436, acc1: 0.147, acc5: 0.366, ep: 5\n",
            "Testing loss: 4.456, acc1: 0.130, acc5: 0.369\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 4.376, acc1: 0.112, acc5: 0.330\n",
            "Training loss: 4.523, acc1: 0.110, acc5: 0.312, ep: 6\n",
            "\n",
            "Validation loss: 4.357, acc1: 0.154, acc5: 0.375, ep: 6\n",
            "Testing loss: 4.366, acc1: 0.141, acc5: 0.379\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 4.288, acc1: 0.141, acc5: 0.354\n",
            "Training loss: 4.427, acc1: 0.125, acc5: 0.331, ep: 7\n",
            "\n",
            "Validation loss: 4.295, acc1: 0.167, acc5: 0.387, ep: 7\n",
            "Testing loss: 4.286, acc1: 0.156, acc5: 0.394\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 4.313, acc1: 0.145, acc5: 0.381\n",
            "Training loss: 4.361, acc1: 0.130, acc5: 0.344, ep: 8\n",
            "\n",
            "Validation loss: 4.209, acc1: 0.170, acc5: 0.391, ep: 8\n",
            "Testing loss: 4.209, acc1: 0.160, acc5: 0.398\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 4.060, acc1: 0.163, acc5: 0.397\n",
            "Training loss: 4.238, acc1: 0.144, acc5: 0.357, ep: 9\n",
            "\n",
            "Validation loss: 4.165, acc1: 0.186, acc5: 0.411, ep: 9\n",
            "Testing loss: 4.201, acc1: 0.169, acc5: 0.406\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 3.988, acc1: 0.171, acc5: 0.401\n",
            "Training loss: 4.170, acc1: 0.151, acc5: 0.365, ep: 10\n",
            "\n",
            "Validation loss: 4.139, acc1: 0.173, acc5: 0.394, ep: 10\n",
            "Testing loss: 4.137, acc1: 0.160, acc5: 0.405\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 3.977, acc1: 0.174, acc5: 0.409\n",
            "Training loss: 4.100, acc1: 0.161, acc5: 0.382, ep: 11\n",
            "\n",
            "Validation loss: 4.132, acc1: 0.176, acc5: 0.405, ep: 11\n",
            "Testing loss: 4.112, acc1: 0.165, acc5: 0.409\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 3.961, acc1: 0.190, acc5: 0.414\n",
            "Training loss: 4.048, acc1: 0.175, acc5: 0.386, ep: 12\n",
            "\n",
            "Validation loss: 4.089, acc1: 0.191, acc5: 0.413, ep: 12\n",
            "Testing loss: 4.084, acc1: 0.174, acc5: 0.421\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 3.794, acc1: 0.193, acc5: 0.425\n",
            "Training loss: 3.945, acc1: 0.180, acc5: 0.396, ep: 13\n",
            "\n",
            "Validation loss: 4.007, acc1: 0.201, acc5: 0.426, ep: 13\n",
            "Testing loss: 3.984, acc1: 0.195, acc5: 0.441\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 3.756, acc1: 0.193, acc5: 0.439\n",
            "Training loss: 3.889, acc1: 0.183, acc5: 0.410, ep: 14\n",
            "\n",
            "Validation loss: 3.948, acc1: 0.227, acc5: 0.459, ep: 14\n",
            "Testing loss: 3.936, acc1: 0.213, acc5: 0.470\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 3.582, acc1: 0.245, acc5: 0.480\n",
            "Training loss: 3.798, acc1: 0.216, acc5: 0.444, ep: 15\n",
            "\n",
            "Validation loss: 3.856, acc1: 0.255, acc5: 0.480, ep: 15\n",
            "Testing loss: 3.870, acc1: 0.245, acc5: 0.493\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 3.442, acc1: 0.272, acc5: 0.502\n",
            "Training loss: 3.667, acc1: 0.233, acc5: 0.453, ep: 16\n",
            "\n",
            "Validation loss: 3.751, acc1: 0.262, acc5: 0.489, ep: 16\n",
            "Testing loss: 3.709, acc1: 0.260, acc5: 0.509\n",
            "\n",
            "Percent: [####################] 100.00% Finished. tr loss: 3.392, acc1: 0.294, acc5: 0.524\n",
            "Training loss: 3.568, acc1: 0.265, acc5: 0.474, ep: 17\n",
            "\n",
            "Validation loss: 3.736, acc1: 0.262, acc5: 0.487, ep: 17\n",
            "Testing loss: 3.705, acc1: 0.265, acc5: 0.506\n",
            "\n",
            "Percent: [###########         ] 56.43%  tr loss: 3.406, acc1: 0.272, acc5: 0.491"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePOiPiMubJz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}