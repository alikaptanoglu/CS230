{\rtf1\ansi\ansicpg936\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red25\green25\blue25;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c12941\c12941\c12941;\cssrgb\c100000\c100000\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 ensemble_embedding Parameter sets: [200, 200, 0.0035, 0.5, 1, 0.5, 30, 100, 130]\
## Building an RNN model\
Tensor("Unigram/Unigram/embedding_lookup:0", shape=(?, 50, 30), dtype=float32)\
Tensor("Bigram/Bigram/embedding_lookup:0", shape=(?, 50, 100), dtype=float32)\
Tensor("Trigram/Trigram/embedding_lookup:0", shape=(?, 50, 130), dtype=float32)\
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "\
model variables ['Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Hidden1/Weights:0', 'Hidden1/Biases:0', 'Output/Weights:0', 'Output/Biases:0']\
## Training\
Percent: [####################] 100.00% Finished. tr loss: 12.893, acc1: 0.165, acc5: 0.307\
Training loss: 5.918, acc1: 0.212, acc5: 0.408, ep: 0\
\
Validation loss: 2.660, acc1: 0.362, acc5: 0.656, ep: 0\
Testing loss: 2.664, acc1: 0.352, acc5: 0.649\
\
Percent: [####################] 100.00% Finished. tr loss: 1.450, acc1: 0.626, acc5: 0.819\
Training loss: 2.664, acc1: 0.372, acc5: 0.655, ep: 1\
\
Validation loss: 2.473, acc1: 0.406, acc5: 0.696, ep: 1\
Testing loss: 2.445, acc1: 0.394, acc5: 0.687\
\
Percent: [####################] 100.00% Finished. tr loss: 1.330, acc1: 0.654, acc5: 0.854\
Training loss: 2.381, acc1: 0.416, acc5: 0.710, ep: 2\
\
Validation loss: 2.348, acc1: 0.416, acc5: 0.723, ep: 2\
Testing loss: 2.338, acc1: 0.413, acc5: 0.705\
\
Percent: [####################] 100.00% Finished. tr loss: 1.344, acc1: 0.669, acc5: 0.839\
Training loss: 2.253, acc1: 0.427, acc5: 0.722, ep: 3\
\
Validation loss: 2.280, acc1: 0.432, acc5: 0.729, ep: 3\
Testing loss: 2.224, acc1: 0.432, acc5: 0.727\
\
Percent: [####################] 100.00% Finished. tr loss: 1.141, acc1: 0.665, acc5: 0.906\
Training loss: 2.102, acc1: 0.455, acc5: 0.771, ep: 4\
\
Validation loss: 2.217, acc1: 0.441, acc5: 0.750, ep: 4\
Testing loss: 2.168, acc1: 0.444, acc5: 0.740\
\
Percent: [####################] 100.00% Finished. tr loss: 1.105, acc1: 0.720, acc5: 0.882\
Training loss: 1.998, acc1: 0.474, acc5: 0.775, ep: 5\
\
Validation loss: 2.165, acc1: 0.446, acc5: 0.766, ep: 5\
Testing loss: 2.112, acc1: 0.446, acc5: 0.748\
\
Percent: [####################] 100.00% Finished. tr loss: 1.081, acc1: 0.701, acc5: 0.898\
Training loss: 1.916, acc1: 0.493, acc5: 0.793, ep: 6\
\
Validation loss: 2.136, acc1: 0.458, acc5: 0.769, ep: 6\
Testing loss: 2.109, acc1: 0.449, acc5: 0.756\
\
Percent: [####################] 100.00% Finished. tr loss: 1.057, acc1: 0.701, acc5: 0.917\
Training loss: 1.827, acc1: 0.510, acc5: 0.812, ep: 7\
\
Validation loss: 2.098, acc1: 0.468, acc5: 0.780, ep: 7\
Testing loss: 2.064, acc1: 0.474, acc5: 0.771\
\
Percent: [####################] 100.00% Finished. tr loss: 0.990, acc1: 0.717, acc5: 0.925\
Training loss: 1.735, acc1: 0.527, acc5: 0.823, ep: 8\
\
Validation loss: 2.104, acc1: 0.449, acc5: 0.781, ep: 8\
Testing loss: 2.061, acc1: 0.471, acc5: 0.782\
\
Percent: [####################] 100.00% Finished. tr loss: 0.899, acc1: 0.776, acc5: 0.937\
Training loss: 1.651, acc1: 0.552, acc5: 0.839, ep: 9\
\
Validation loss: 2.043, acc1: 0.471, acc5: 0.796, ep: 9\
Testing loss: 2.025, acc1: 0.484, acc5: 0.786\
\
Percent: [####################] 100.00% Finished. tr loss: 0.890, acc1: 0.768, acc5: 0.937\
Training loss: 1.567, acc1: 0.571, acc5: 0.855, ep: 10\
\
Validation loss: 2.046, acc1: 0.482, acc5: 0.793, ep: 10\
Testing loss: 2.033, acc1: 0.483, acc5: 0.787\
\
Percent: [####################] 100.00% Finished. tr loss: 0.867, acc1: 0.744, acc5: 0.941\
Training loss: 1.486, acc1: 0.584, acc5: 0.875, ep: 11\
\
Validation loss: 2.054, acc1: 0.475, acc5: 0.792, ep: 11\
Testing loss: 2.052, acc1: 0.490, acc5: 0.781\
\
Percent: [####################] 100.00% Finished. tr loss: 0.812, acc1: 0.760, acc5: 0.949\
Training loss: 1.416, acc1: 0.603, acc5: 0.878, ep: 12\
\
Validation loss: 2.038, acc1: 0.481, acc5: 0.793, ep: 12\
Testing loss: 2.006, acc1: 0.492, acc5: 0.780\
\
Percent: [####################] 100.00% Finished. tr loss: 0.714, acc1: 0.791, acc5: 0.965\
Training loss: 1.365, acc1: 0.607, acc5: 0.891, ep: 13\
\
Validation loss: 2.050, acc1: 0.495, acc5: 0.807, ep: 13\
Testing loss: 2.012, acc1: 0.510, acc5: 0.785\
\
Percent: [####################] 100.00% Finished. tr loss: 0.668, acc1: 0.787, acc5: 0.969\
Training loss: 1.256, acc1: 0.638, acc5: 0.909, ep: 14\
\
Validation loss: 2.114, acc1: 0.483, acc5: 0.792, ep: 14\
Testing loss: 2.068, acc1: 0.494, acc5: 0.784\
\
Percent: [####################] 100.00% Finished. tr loss: 0.631, acc1: 0.791, acc5: 0.961\
Training loss: 1.197, acc1: 0.646, acc5: 0.906, ep: 15\
\
Validation loss: 2.145, acc1: 0.485, acc5: 0.789, ep: 15\
Testing loss: 2.075, acc1: 0.498, acc5: 0.794\
\
Percent: [####################] 100.00% Finished. tr loss: 0.619, acc1: 0.811, acc5: 0.965\
Training loss: 1.128, acc1: 0.661, acc5: 0.924, ep: 16\
\
Validation loss: 2.161, acc1: 0.490, acc5: 0.793, ep: 16\
Testing loss: 2.095, acc1: 0.503, acc5: 0.799\
\
Percent: [####################] 100.00% Finished. tr loss: 0.635, acc1: 0.811, acc5: 0.965\
Training loss: 1.078, acc1: 0.675, acc5: 0.933, ep: 17\
\
Validation loss: 2.206, acc1: 0.497, acc5: 0.792, ep: 17\
Testing loss: 2.217, acc1: 0.498, acc5: 0.787\
\
Percent: [####################] 100.00% Finished. tr loss: 0.512, acc1: 0.827, acc5: 0.992\
Training loss: 1.008, acc1: 0.688, acc5: 0.937, ep: 18\
\
Validation loss: 2.179, acc1: 0.498, acc5: 0.793, ep: 18\
Testing loss: 2.195, acc1: 0.497, acc5: 0.792\
\
Percent: [####################] 100.00% Finished. tr loss: 0.550, acc1: 0.843, acc5: 0.972\
Training loss: 0.997, acc1: 0.698, acc5: 0.941, ep: 19\
\
Validation loss: 2.248, acc1: 0.479, acc5: 0.795, ep: 19\
Testing loss: 2.301, acc1: 0.501, acc5: 0.776\
\
Percent: [####################] 100.00% Finished. tr loss: 0.494, acc1: 0.858, acc5: 0.976\
Training loss: 0.944, acc1: 0.719, acc5: 0.953, ep: 20\
\
Validation loss: 2.335, acc1: 0.479, acc5: 0.794, ep: 20\
Testing loss: 2.361, acc1: 0.481, acc5: 0.789\
\
Percent: [####################] 100.00% Finished. tr loss: 0.494, acc1: 0.839, acc5: 0.980\
Training loss: 0.846, acc1: 0.736, acc5: 0.955, ep: 21\
\
Validation loss: 2.372, acc1: 0.491, acc5: 0.794, ep: 21\
Testing loss: 2.443, acc1: 0.487, acc5: 0.785\
\
Percent: [####################] 100.00% Finished. tr loss: 0.374, acc1: 0.886, acc5: 0.980\
Training loss: 0.797, acc1: 0.740, acc5: 0.965, ep: 22\
\
Validation loss: 2.408, acc1: 0.484, acc5: 0.788, ep: 22\
Testing loss: 2.421, acc1: 0.492, acc5: 0.792\
\
Percent: [####################] 100.00% Finished. tr loss: 0.335, acc1: 0.894, acc5: 0.992\
Training loss: 0.723, acc1: 0.770, acc5: 0.973, ep: 23\
\
Validation loss: 2.406, acc1: 0.476, acc5: 0.799, ep: 23\
Testing loss: 2.417, acc1: 0.504, acc5: 0.791\
\
Early stopping applied\
\
Testing loss: 2.417, acc1: 0.504, acc5: 0.791\
ensemble_embedding Parameter sets: [200, 200, 0.0035, 0.5, 1, 0.5, 30, 100, 130]\
## Building an RNN model\
Tensor("Unigram/Unigram/embedding_lookup:0", shape=(?, 50, 30), dtype=float32)\
Tensor("Bigram/Bigram/embedding_lookup:0", shape=(?, 50, 100), dtype=float32)\
Tensor("Trigram/Trigram/embedding_lookup:0", shape=(?, 50, 130), dtype=float32)\
model variables ['Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Hidden1/Weights:0', 'Hidden1/Biases:0', 'Output/Weights:0', 'Output/Biases:0']\
## Training\
Percent: [####################] 100.00% Finished. tr loss: 9.888, acc1: 0.134, acc5: 0.311\
Training loss: 6.085, acc1: 0.211, acc5: 0.409, ep: 0\
\
Validation loss: 2.698, acc1: 0.341, acc5: 0.664, ep: 0\
Testing loss: 2.652, acc1: 0.364, acc5: 0.655\
\
Percent: [####################] 100.00% Finished. tr loss: 1.720, acc1: 0.563, acc5: 0.768\
Training loss: 2.782, acc1: 0.352, acc5: 0.639, ep: 1\
\
Validation loss: 2.504, acc1: 0.376, acc5: 0.685, ep: 1\
Testing loss: 2.449, acc1: 0.400, acc5: 0.679\
\
Percent: [####################] 100.00% Finished. tr loss: 1.483, acc1: 0.630, acc5: 0.819\
Training loss: 2.452, acc1: 0.410, acc5: 0.693, ep: 2\
\
Validation loss: 2.415, acc1: 0.397, acc5: 0.709, ep: 2\
Testing loss: 2.359, acc1: 0.415, acc5: 0.711\
\
Percent: [####################] 100.00% Finished. tr loss: 1.337, acc1: 0.630, acc5: 0.866\
Training loss: 2.309, acc1: 0.427, acc5: 0.723, ep: 3\
\
Validation loss: 2.334, acc1: 0.416, acc5: 0.731, ep: 3\
Testing loss: 2.295, acc1: 0.423, acc5: 0.730\
\
Percent: [####################] 100.00% Finished. tr loss: 1.193, acc1: 0.705, acc5: 0.894\
Training loss: 2.149, acc1: 0.456, acc5: 0.758, ep: 4\
\
Validation loss: 2.277, acc1: 0.421, acc5: 0.750, ep: 4\
Testing loss: 2.249, acc1: 0.437, acc5: 0.729\
\
Percent: [####################] 100.00% Finished. tr loss: 1.228, acc1: 0.654, acc5: 0.890\
Training loss: 2.079, acc1: 0.473, acc5: 0.767, ep: 5\
\
Validation loss: 2.253, acc1: 0.434, acc5: 0.750, ep: 5\
Testing loss: 2.208, acc1: 0.451, acc5: 0.750\
\
Percent: [####################] 100.00% Finished. tr loss: 1.065, acc1: 0.693, acc5: 0.913\
Training loss: 1.940, acc1: 0.494, acc5: 0.796, ep: 6\
\
Validation loss: 2.221, acc1: 0.439, acc5: 0.755, ep: 6\
Testing loss: 2.170, acc1: 0.458, acc5: 0.750\
\
Percent: [####################] 100.00% Finished. tr loss: 1.032, acc1: 0.732, acc5: 0.909\
Training loss: 1.873, acc1: 0.510, acc5: 0.800, ep: 7\
\
Validation loss: 2.185, acc1: 0.445, acc5: 0.762, ep: 7\
Testing loss: 2.156, acc1: 0.454, acc5: 0.756\
\
Percent: [####################] 100.00% Finished. tr loss: 0.983, acc1: 0.724, acc5: 0.913\
Training loss: 1.726, acc1: 0.530, acc5: 0.829, ep: 8\
\
Validation loss: 2.219, acc1: 0.442, acc5: 0.765, ep: 8\
Testing loss: 2.182, acc1: 0.466, acc5: 0.753\
\
Percent: [####################] 100.00% Finished. tr loss: 0.935, acc1: 0.728, acc5: 0.909\
Training loss: 1.675, acc1: 0.541, acc5: 0.835, ep: 9\
\
Validation loss: 2.194, acc1: 0.453, acc5: 0.777, ep: 9\
Testing loss: 2.137, acc1: 0.480, acc5: 0.765\
\
Percent: [####################] 100.00% Finished. tr loss: 0.876, acc1: 0.783, acc5: 0.941\
Training loss: 1.627, acc1: 0.564, acc5: 0.841, ep: 10\
\
Validation loss: 2.209, acc1: 0.458, acc5: 0.771, ep: 10\
Testing loss: 2.142, acc1: 0.483, acc5: 0.755\
\
Percent: [####################] 100.00% Finished. tr loss: 0.859, acc1: 0.752, acc5: 0.925\
Training loss: 1.509, acc1: 0.577, acc5: 0.863, ep: 11\
\
Validation loss: 2.199, acc1: 0.464, acc5: 0.769, ep: 11\
Testing loss: 2.139, acc1: 0.481, acc5: 0.776\
\
Percent: [####################] 100.00% Finished. tr loss: 0.700, acc1: 0.787, acc5: 0.961\
Training loss: 1.394, acc1: 0.608, acc5: 0.884, ep: 12\
\
Validation loss: 2.298, acc1: 0.473, acc5: 0.770, ep: 12\
Testing loss: 2.263, acc1: 0.473, acc5: 0.772\
\
Percent: [####################] 100.00% Finished. tr loss: 0.707, acc1: 0.768, acc5: 0.953\
Training loss: 1.368, acc1: 0.603, acc5: 0.891, ep: 13\
\
Validation loss: 2.289, acc1: 0.469, acc5: 0.777, ep: 13\
Testing loss: 2.231, acc1: 0.491, acc5: 0.787\
\
Percent: [####################] 100.00% Finished. tr loss: 0.639, acc1: 0.815, acc5: 0.953\
Training loss: 1.301, acc1: 0.620, acc5: 0.900, ep: 14\
\
Validation loss: 2.314, acc1: 0.463, acc5: 0.780, ep: 14\
Testing loss: 2.250, acc1: 0.489, acc5: 0.780\
\
Percent: [####################] 100.00% Finished. tr loss: 0.683, acc1: 0.799, acc5: 0.969\
Training loss: 1.227, acc1: 0.634, acc5: 0.914, ep: 15\
\
Validation loss: 2.324, acc1: 0.465, acc5: 0.768, ep: 15\
Testing loss: 2.289, acc1: 0.487, acc5: 0.787\
\
Percent: [####################] 100.00% Finished. tr loss: 0.584, acc1: 0.807, acc5: 0.976\
Training loss: 1.128, acc1: 0.670, acc5: 0.927, ep: 16\
\
Validation loss: 2.377, acc1: 0.459, acc5: 0.775, ep: 16\
Testing loss: 2.308, acc1: 0.477, acc5: 0.783\
\
Percent: [####################] 100.00% Finished. tr loss: 0.665, acc1: 0.803, acc5: 0.969\
Training loss: 1.065, acc1: 0.685, acc5: 0.929, ep: 17\
\
Validation loss: 2.406, acc1: 0.460, acc5: 0.776, ep: 17\
Testing loss: 2.357, acc1: 0.481, acc5: 0.783\
\
Early stopping applied\
\
Testing loss: 2.357, acc1: 0.481, acc5: 0.783\
ensemble_embedding Parameter sets: [200, 200, 0.0035, 0.5, 1, 0.5, 30, 100, 130]\
## Building an RNN model\
Tensor("Unigram/Unigram/embedding_lookup:0", shape=(?, 50, 30), dtype=float32)\
Tensor("Bigram/Bigram/embedding_lookup:0", shape=(?, 50, 100), dtype=float32)\
Tensor("Trigram/Trigram/embedding_lookup:0", shape=(?, 50, 130), dtype=float32)\
model variables ['Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Hidden1/Weights:0', 'Hidden1/Biases:0', 'Output/Weights:0', 'Output/Biases:0']\
## Training\
Percent: [####################] 100.00% Finished. tr loss: 9.910, acc1: 0.079, acc5: 0.272\
Training loss: 6.291, acc1: 0.182, acc5: 0.394, ep: 0\
\
Validation loss: 2.649, acc1: 0.341, acc5: 0.676, ep: 0\
Testing loss: 2.632, acc1: 0.366, acc5: 0.670\
\
Percent: [####################] 100.00% Finished. tr loss: 1.796, acc1: 0.563, acc5: 0.787\
Training loss: 2.834, acc1: 0.338, acc5: 0.636, ep: 1\
\
Validation loss: 2.483, acc1: 0.396, acc5: 0.709, ep: 1\
Testing loss: 2.475, acc1: 0.396, acc5: 0.682\
\
Percent: [####################] 100.00% Finished. tr loss: 1.528, acc1: 0.602, acc5: 0.831\
Training loss: 2.506, acc1: 0.394, acc5: 0.691, ep: 2\
\
Validation loss: 2.369, acc1: 0.407, acc5: 0.722, ep: 2\
Testing loss: 2.355, acc1: 0.405, acc5: 0.709\
\
Percent: [####################] 100.00% Finished. tr loss: 1.335, acc1: 0.669, acc5: 0.850\
Training loss: 2.293, acc1: 0.437, acc5: 0.721, ep: 3\
\
Validation loss: 2.280, acc1: 0.431, acc5: 0.749, ep: 3\
Testing loss: 2.300, acc1: 0.413, acc5: 0.726\
\
Percent: [####################] 100.00% Finished. tr loss: 1.329, acc1: 0.661, acc5: 0.854\
Training loss: 2.174, acc1: 0.454, acc5: 0.747, ep: 4\
\
Validation loss: 2.225, acc1: 0.434, acc5: 0.758, ep: 4\
Testing loss: 2.255, acc1: 0.425, acc5: 0.743\
\
Percent: [####################] 100.00% Finished. tr loss: 1.211, acc1: 0.677, acc5: 0.886\
Training loss: 2.041, acc1: 0.488, acc5: 0.767, ep: 5\
\
Validation loss: 2.204, acc1: 0.428, acc5: 0.758, ep: 5\
Testing loss: 2.231, acc1: 0.434, acc5: 0.739\
\
Percent: [####################] 100.00% Finished. tr loss: 1.098, acc1: 0.709, acc5: 0.902\
Training loss: 1.953, acc1: 0.491, acc5: 0.794, ep: 6\
\
Validation loss: 2.147, acc1: 0.446, acc5: 0.768, ep: 6\
Testing loss: 2.193, acc1: 0.437, acc5: 0.746\
\
Percent: [####################] 100.00% Finished. tr loss: 1.150, acc1: 0.677, acc5: 0.898\
Training loss: 1.900, acc1: 0.498, acc5: 0.803, ep: 7\
\
Validation loss: 2.136, acc1: 0.453, acc5: 0.787, ep: 7\
Testing loss: 2.165, acc1: 0.447, acc5: 0.762\
\
Percent: [####################] 100.00% Finished. tr loss: 1.000, acc1: 0.717, acc5: 0.921\
Training loss: 1.805, acc1: 0.521, acc5: 0.814, ep: 8\
\
Validation loss: 2.108, acc1: 0.453, acc5: 0.781, ep: 8\
Testing loss: 2.151, acc1: 0.453, acc5: 0.758\
\
Percent: [####################] 100.00% Finished. tr loss: 1.034, acc1: 0.736, acc5: 0.906\
Training loss: 1.711, acc1: 0.547, acc5: 0.833, ep: 9\
\
Validation loss: 2.108, acc1: 0.466, acc5: 0.796, ep: 9\
Testing loss: 2.156, acc1: 0.444, acc5: 0.774\
\
Percent: [####################] 100.00% Finished. tr loss: 0.908, acc1: 0.705, acc5: 0.945\
Training loss: 1.634, acc1: 0.552, acc5: 0.850, ep: 10\
\
Validation loss: 2.181, acc1: 0.460, acc5: 0.788, ep: 10\
Testing loss: 2.221, acc1: 0.446, acc5: 0.762\
\
Percent: [####################] 100.00% Finished. tr loss: 0.829, acc1: 0.724, acc5: 0.941\
Training loss: 1.543, acc1: 0.571, acc5: 0.864, ep: 11\
\
Validation loss: 2.159, acc1: 0.467, acc5: 0.794, ep: 11\
Testing loss: 2.190, acc1: 0.458, acc5: 0.761\
\
Percent: [####################] 100.00% Finished. tr loss: 0.827, acc1: 0.756, acc5: 0.949\
Training loss: 1.456, acc1: 0.590, acc5: 0.880, ep: 12\
\
Validation loss: 2.187, acc1: 0.463, acc5: 0.786, ep: 12\
Testing loss: 2.222, acc1: 0.461, acc5: 0.774\
\
Percent: [####################] 100.00% Finished. tr loss: 0.771, acc1: 0.795, acc5: 0.941\
Training loss: 1.390, acc1: 0.609, acc5: 0.882, ep: 13\
\
Validation loss: 2.220, acc1: 0.464, acc5: 0.785, ep: 13\
Testing loss: 2.244, acc1: 0.460, acc5: 0.764\
\
Percent: [####################] 100.00% Finished. tr loss: 0.688, acc1: 0.791, acc5: 0.953\
Training loss: 1.319, acc1: 0.629, acc5: 0.894, ep: 14\
\
Validation loss: 2.222, acc1: 0.459, acc5: 0.796, ep: 14\
Testing loss: 2.268, acc1: 0.466, acc5: 0.771\
\
Percent: [####################] 100.00% Finished. tr loss: 0.654, acc1: 0.819, acc5: 0.957\
Training loss: 1.223, acc1: 0.646, acc5: 0.906, ep: 15\
\
Validation loss: 2.235, acc1: 0.457, acc5: 0.791, ep: 15\
Testing loss: 2.263, acc1: 0.464, acc5: 0.772\
\
Percent: [####################] 100.00% Finished. tr loss: 0.721, acc1: 0.772, acc5: 0.953\
Training loss: 1.160, acc1: 0.658, acc5: 0.922, ep: 16\
\
Validation loss: 2.293, acc1: 0.461, acc5: 0.789, ep: 16\
Testing loss: 2.334, acc1: 0.464, acc5: 0.770\
\
Early stopping applied\
\
Testing loss: 2.334, acc1: 0.464, acc5: 0.770\
ensemble_embedding Parameter sets: [200, 200, 0.0035, 0.5, 1, 0.5, 30, 100, 130]\
## Building an RNN model\
Tensor("Unigram/Unigram/embedding_lookup:0", shape=(?, 50, 30), dtype=float32)\
Tensor("Bigram/Bigram/embedding_lookup:0", shape=(?, 50, 100), dtype=float32)\
Tensor("Trigram/Trigram/embedding_lookup:0", shape=(?, 50, 130), dtype=float32)\
model variables ['Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Unigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Bigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0', 'Trigram/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0', 'Hidden1/Weights:0', 'Hidden1/Biases:0', 'Output/Weights:0', 'Output/Biases:0']\
## Training\
Percent: [####################] 100.00% Finished. tr loss: 5.656, acc1: 0.150, acc5: 0.480\
Training loss: 5.795, acc1: 0.207, acc5: 0.416, ep: 0\
\
Validation loss: 2.702, acc1: 0.342, acc5: 0.653, ep: 0\
Testing loss: 2.649, acc1: 0.355, acc5: 0.651\
\
Percent: [####################] 100.00% Finished. tr loss: 1.784, acc1: 0.575, acc5: 0.780\
Training loss: 2.833, acc1: 0.339, acc5: 0.623, ep: 1\
\
Validation loss: 2.526, acc1: 0.375, acc5: 0.692, ep: 1\
Testing loss: 2.496, acc1: 0.373, acc5: 0.683\
\
Percent: [####################] 100.00% Finished. tr loss: 1.510, acc1: 0.622, acc5: 0.823\
Training loss: 2.478, acc1: 0.401, acc5: 0.693, ep: 2\
\
Validation loss: 2.400, acc1: 0.400, acc5: 0.713, ep: 2\
Testing loss: 2.366, acc1: 0.401, acc5: 0.717\
\
Percent: [####################] 100.00% Finished. tr loss: 1.380, acc1: 0.638, acc5: 0.846\
Training loss: 2.286, acc1: 0.437, acc5: 0.727, ep: 3\
\
Validation loss: 2.313, acc1: 0.425, acc5: 0.729, ep: 3\
Testing loss: 2.279, acc1: 0.418, acc5: 0.740\
\
Percent: [####################] 100.00% Finished. tr loss: 1.255, acc1: 0.657, acc5: 0.874\
Training loss: 2.177, acc1: 0.447, acc5: 0.748, ep: 4\
\
Validation loss: 2.283, acc1: 0.431, acc5: 0.749, ep: 4\
Testing loss: 2.240, acc1: 0.437, acc5: 0.744\
\
Percent: [####################] 100.00% Finished. tr loss: 1.287, acc1: 0.657, acc5: 0.850\
Training loss: 2.071, acc1: 0.467, acc5: 0.768, ep: 5\
\
Validation loss: 2.238, acc1: 0.446, acc5: 0.753, ep: 5\
Testing loss: 2.179, acc1: 0.441, acc5: 0.758\
\
Percent: [####################] 100.00% Finished. tr loss: 1.203, acc1: 0.665, acc5: 0.894\
Training loss: 1.993, acc1: 0.479, acc5: 0.778, ep: 6\
\
Validation loss: 2.186, acc1: 0.461, acc5: 0.775, ep: 6\
Testing loss: 2.160, acc1: 0.461, acc5: 0.762\
\
Percent: [####################] 100.00% Finished. tr loss: 1.074, acc1: 0.732, acc5: 0.898\
Training loss: 1.887, acc1: 0.502, acc5: 0.799, ep: 7\
\
Validation loss: 2.165, acc1: 0.462, acc5: 0.776, ep: 7\
Testing loss: 2.162, acc1: 0.456, acc5: 0.767\
\
Percent: [####################] 100.00% Finished. tr loss: 1.010, acc1: 0.720, acc5: 0.921\
Training loss: 1.796, acc1: 0.516, acc5: 0.816, ep: 8\
\
Validation loss: 2.140, acc1: 0.460, acc5: 0.773, ep: 8\
Testing loss: 2.139, acc1: 0.461, acc5: 0.766\
\
Percent: [####################] 100.00% Finished. tr loss: 0.935, acc1: 0.740, acc5: 0.925\
Training loss: 1.700, acc1: 0.531, acc5: 0.834, ep: 9\
\
Validation loss: 2.120, acc1: 0.464, acc5: 0.782, ep: 9\
Testing loss: 2.110, acc1: 0.481, acc5: 0.774\
\
Percent: [####################] 100.00% Finished. tr loss: 0.856, acc1: 0.772, acc5: 0.937\
Training loss: 1.598, acc1: 0.572, acc5: 0.854, ep: 10\
\
Validation loss: 2.134, acc1: 0.461, acc5: 0.791, ep: 10\
Testing loss: 2.114, acc1: 0.459, acc5: 0.781\
\
Percent: [#########           ] 44.63%  tr loss: 1.664, acc1: 0.523, acc5: 0.847}